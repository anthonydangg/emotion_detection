# -*- coding: utf-8 -*-
"""Capstone_baseline_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tpIVXTCR7brHEB0ZbmL9lY0aVfuCnxvt
"""

#!pip install --upgrade pytorch-pretrained-vit

import json
from PIL import Image

import torch
from torchvision import transforms

#from pytorch_pretrained_vit import ViT

import cv2
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
import random

# Load ViT
from pytorch_pretrained_vit import ViT
model = ViT('B_16_imagenet1k', pretrained=True)
model.eval()

from google.colab import files

# Upload the zip file
uploaded = files.upload()

#resizes and normalizes image
transformer = transforms.Compose([transforms.Resize((384, 384)),transforms.ToTensor(),transforms.Normalize(0.5, 0.5),])

import zipfile
import os

# Specify the name of the uploaded zip file
zip_file_name = 'archive (2).zip'

# Specify the folder where you want to extract the contents
extract_folder_path = '/content/images/'

# Open the zip file and extract its contents
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extract_folder_path)

print(f"Contents of {zip_file_name} have been extracted to {extract_folder_path}")

files_in_folder = os.listdir(extract_folder_path)

# Print the list of files
print("Files in the extracted folder:")
for file_name in files_in_folder:
    print(file_name)

og_path = extract_folder_path + files_in_folder[1]
og_path

#labels
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

!pip install deepface

from deepface import DeepFace

import deepface

model = DeepFace.build_model("Emotion")
model

model

saved_model_dir = f"./deepface_lite"
model.save(saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()

with open(f"deepface_tflite_1", "wb") as f:
    f.write(tflite_model)

lst_emotion_probability = []

for emo_class in classes:
    path = os.path.join(og_path, emo_class)
    for img in os.listdir(path):
        img_arr = cv2.imread(os.path.join(path, img))
        plt.imshow(cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB))

        result = DeepFace.analyze(img_arr, actions = ['emotion'], enforce_detection = False)

        dictionary_emotions = result[0]['emotion']
        lst_emotion_probability.append(max(dictionary_emotions,  key=dictionary_emotions.get))



lst_emotion_probability

train_path = extract_folder_path + files_in_folder[1]
train_path

#preporcessing images
image_tensors = []
labels = []
for emo_class in classes:
    path = os.path.join(train_path, emo_class)
    for i in os.listdir(path):
        print(path + i)
        #img_tensor = torch.stack([transformer(Image.open(os.path.join(path, i)).convert("RGB"))])
        #image_tensors.append(img_tensor)
        labels.append(emo_class)

labels

len(lst_emotion_probability)

matching_elements = sum(x == y for x, y in zip(labels, lst_emotion_probability))

# Calculate the percentage of matches
percentage_matches = (matching_elements / max(len(labels), len(lst_emotion_probability))) * 100

percentage_matches

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a confusion matrix
conf_matrix = confusion_matrix(pd.Series(labels), pd.Series(lst_emotion_probability))

# Plot the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['surprise', 'sad', 'neutral', 'happy', 'fear', 'disgust', 'angry'], yticklabels= ['surprise', 'sad', 'neutral', 'happy', 'fear', 'disgust', 'angry'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()



"""**BELOW IS A SELF CREATED SEQUANTIAL 3 LAYER CNN MDOEL **"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, AveragePooling2D

num_classes = 7
batch_size = 256
epochs = 5

model = Sequential()

#1st convolution layer
model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))
model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))

#2nd convolution layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))

#3rd convolution layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))

model.add(Flatten())

#fully connected neural networks
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.4))

model.add(Dense(num_classes, activation='softmax'))

emotion_dict_reverse = {
    'angry': 0,
    'disgust': 1,
    'fear': 2,
    'happy': 3,
    'sad': 4,
    'surprise': 5,
    'neutral': 6
}

from keras.preprocessing import image
mage_tensors = []
labels = []
x_train = []
y_train = []
total_train_samples = 0
for emo_class in classes:
    path = os.path.join(og_path, emo_class)
    for i in os.listdir(path):
        #img_tensor = torch.stack([transformer(Image.open(os.path.join(path, i)).convert("RGB"))])
        img = image.load_img(os.path.join(path, i), grayscale=True, target_size=(48, 48))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis = 0)

        x /= 255
        #image_tensors.append(img_tensor)
        labels.append(emo_class)

        x_train.append(x)
        y_train.append(emotion_dict_reverse[emo_class])

        total_train_samples += 1

x_train = np.array(x_train)
y_train = np.array(y_train)

x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)
x_train = x_train.astype('float32')

from keras.preprocessing.image import ImageDataGenerator
gen = ImageDataGenerator()
train_generator = gen.flow(x_train, y_train, batch_size=batch_size)

from tensorflow.keras.optimizers import Adam
model.compile(loss='sparse_categorical_crossentropy',
              optimizer= Adam(),
              metrics=['accuracy'])

from google.colab import files

# Upload the zip file
uploaded = files.upload()

model.fit(train_generator, steps_per_epoch=80, epochs=epochs)
model.load_weights('facial_expression_model_weights.h5')

#processing test images
test_path = extract_folder_path + files_in_folder[1]
labels = []
x_test = []
y_test = []
for emo_class in classes:
    path = os.path.join(test_path, emo_class)
    for i in os.listdir(path):
        #img_tensor = torch.stack([transformer(Image.open(os.path.join(path, i)).convert("RGB"))])
        img = image.load_img(os.path.join(path, i), grayscale=True, target_size=(48, 48))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis = 0)

        x /= 255
        labels.append(emo_class)

        x_test.append(x)
        y_test.append(emotion_dict_reverse[emo_class])

x_test = np.array(x_test)
y_test = np.array(y_test)

x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)
x_test = x_test.astype('float32')

total_train_samples

# evaluation on test set
score = model.evaluate(x_test, y_test)
print('Test loss:', score[0])
print('Test accuracy:', 100*score[1])





from google.colab import files

# Upload the zip file
uploaded = files.upload()

# Specify the name of the uploaded zip file
import zipfile

# List of zip file names
zip_file_names = ['40males.zip', '40females.zip']

# Specify the folder where you want to extract the contents
extract_folder_path = '/images/'

# Iterate through each zip file
for zip_file_name in zip_file_names:
    # Open the zip file and extract its contents
    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
        zip_ref.extractall(extract_folder_path)

    print(f"Contents of {zip_file_name} have been extracted to {extract_folder_path}")

"""/Users/varundinesh/Downloads/40Males/Happy/S92_8yoM/S92_8yoM_HappyTeeth_right 2.jpg"""

darthmuth_path_males = extract_folder_path + files_in_folder[0]
darthmuth_path_females = extract_folder_path + files_in_folder[1]

labels = []
x_total_males = []
y_total_males = []
for emo_class in classes:
    path = os.path.join(darthmuth_path_males, emo_class)
    for i in os.listdir(path):
        img = image.load_img(os.path.join(path, i), grayscale=True, target_size=(48, 48))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis = 0)

        x /= 255
        labels.append(emo_class)

        x_total_males.append(x)
        y_total_males.append(emotion_dict_reverse[emo_class])

labels = []
x_total_females = []
y_total_females = []
for emo_class in classes:
    path = os.path.join(darthmuth_path_females, emo_class)
    for i in os.listdir(path):
        img = image.load_img(os.path.join(path, i), grayscale=True, target_size=(48, 48))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis = 0)

        x /= 255
        labels.append(emo_class)

        x_total_females.append(x)
        y_total_females.append(emotion_dict_reverse[emo_class])